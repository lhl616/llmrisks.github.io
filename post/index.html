<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Posts | Risks (and Benefits) of Generative AI and Large Language Models</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="https://llmrisks.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/fonts.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/finite.css">
    <link rel="shortcut icon" href="/images/uva.png">  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Computer Science</br>University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      

<div class="container">
<div class="content">
<div class="row">
    
    
    <h2><a href="/week13/">Week 13: Regulating Dangerous Technologies</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 November 2023</time>
  </span>
  
</div>


The slides are here: Regulating Dangerous Technologies (I&rsquo;ve included some slides in the posted slides that I didn&rsquo;t present in class but you might find interesting, including some excerpts from a talk I gave in 2018 on Mutually Assured Destruction and the Impending AI Apocalypse.)
Since one of the groups made the analogy to tobacco products, I also will take the liberty of pointing to a talk I gave at Google making a similar analogy: The Dragon in the Room.
<p class="text-right"><a href="/week13/">Read More…</a></p>
	

    
    <h2><a href="/week11/">Week 11: Watermarking on Generative Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 November 2023</time>
  </span>
  
</div>


Presenting Team: Tseganesh Beyene Kebede, Zihan Guan, Xindi Guo, Mengxuan Hu
Blogging Team: Ajwa Shahid, Caroline Gihlstorf, Changhong Yang, Hyeongjin Kim, Sarah Boyce
Monday, November 6: Watermarking LLM Outputs Recent instances of AI-generated text passing for human text and the writing of students being misattributed to AI suggest the need for a tool to distinguish between human-written and AI-generated text. The presenters also noted that the increase in the amount of AI-generated text online is a risk for training future LLMs on this data.
<p class="text-right"><a href="/week11/">Read More…</a></p>
	

    
    <h2><a href="/week10/">Week 10: Data Selection for LLMs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">3 November 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Presenting Team: Haolin Liu, Xueren Ge, Ji Hyun Kim, Stephanie Schoch 
Blogging Team: Aparna Kishore, Elena Long, Erzhen Hu, Jingping Wan
Monday, 30 October: Data Selection for Fine-tuning LLMs Question: Would more models help? We&rsquo;ve discussed so many risks and issues of GenAI so far and one question is that it can be difficult for us to come up with a possible solution to these problems.
<p class="text-right"><a href="/week10/">Read More…</a></p>
	

    
    <h2><a href="/week9/">Week 9: Interpretability</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-30 00:00:00 &#43;0000 UTC" itemprop="datePublished">30 October 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Presenting Team: Anshuman Suri, Jacob Christopher, Kasra Lekan, Kaylee Liu, My Dinh
Blogging Team: Hamza Khalid, Liu Zhe, Peng Wang, Sikun Guo, Yinhan He, Zhepei Wei
Monday, 23 October: Interpretability: Overview, Limitations, &amp; Challenges Definition of Interpretability  Interpretability in the context of artificial intelligence (AI) and machine learning refers to the extent to which a model&rsquo;s decisions, predictions, or internal workings can be understood and explained by humans.
<p class="text-right"><a href="/week9/">Read More…</a></p>
	

    
    <h2><a href="/week8/">Week 8: Machine Translation</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 October 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Machine Translation (Week 8) Presenting Team: Ajwa Shahid, Caroline Gihlstorf, Changhong Yang, Hyeongjin Kim, Sarah Boyce
Blogging Team: Xindi Guo, Mengxuan Hu, Tseganesh Beyene Kebede, Zihan Guan
Monday, 16 Oct: Diving into the History of Machine Translation Let&rsquo;s kick off this topic with an activity that involves translating an English sentence into a language of your choice and subsequently composing pseudocode to describe the process.
<p class="text-right"><a href="/week8/">Read More…</a></p>
	

    
    <h2><a href="/week7/">Week 7: GANs and DeepFakes</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 October 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Presenting Team: Aparna Kishore, Elena Long, Erzhen Hu, Jingping Wan 
Blogging Team: Haochen Liu, Haolin Liu, Ji Hyun Kim, Stephanie Schoch, Xueren Ge 
Monday, 9 October: Generative Adversarial Networks and DeepFakes Today's topic is how to utilize generative adversarial networks to create fake images and how to identify the images generated by these models.
 Generative Adversarial Network (GAN) is a revolutionary deep learning framework that pits two neural networks against each other in a creative showdown.
<p class="text-right"><a href="/week7/">Read More…</a></p>
	

    
    <h2><a href="/week5/">Week 5: Hallucination</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-04 00:00:00 &#43;0000 UTC" itemprop="datePublished">4 October 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Hallucination (Week 5) Presenting Team: Liu Zhe, Peng Wang, Sikun Guo, Yinhan He, Zhepei Wei
Blogging Team: Anshuman Suri, Jacob Christopher, Kasra Lekan, Kaylee Liu, My Dinh
Wednesday, September 27th: Intro to Hallucination      People Hallucinate Too         Hallucination Definition  There are three types of hallucinations according to the “Siren's Song in the AI Ocean” paper:  Input-conflict: This subcategory of hallucinations deviates from user input.
<p class="text-right"><a href="/week5/">Read More…</a></p>
	

    
    <h2><a href="/week4/">Week 4: Capabilities of LLMs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-25 00:00:00 &#43;0000 UTC" itemprop="datePublished">25 September 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Capabilities of LLMs (Week 4) Presenting Team: Xindi Guo, Mengxuan Hu, Tseganesh Beyene Kebede, Zihan Guan
Blogging Team: Ajwa Shahid, Caroline Gihlstorf, Changhong Yang, Hyeongjin Kim, Sarah Boyce
Monday, September 18 Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu. Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. April 2023. https://arxiv.org/abs/2304.13712
<p class="text-right"><a href="/week4/">Read More…</a></p>
	

    
    <h2><a href="/week3/">Week 3: Prompting and Bias</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">18 September 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Prompt Engineering (Week 3) Presenting Team: Haolin Liu, Xueren Ge, Ji Hyun Kim, Stephanie Schoch 
Blogging Team: Aparna Kishore, Erzhen Hu, Elena Long, Jingping Wan
 (Monday, 09/11/2023) Prompt Engineering  Warm-up questions What is Prompt Engineering? How is prompt-based learning different from traditional supervised learning? In-context learning and different types of prompts What is the difference between prompts and fine-tuning? When is the best to use prompts vs fine-tuning?
<p class="text-right"><a href="/week3/">Read More…</a></p>
	

    
    <h2><a href="/week2/">Week 2: Alignment</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-11 00:00:00 &#43;0000 UTC" itemprop="datePublished">11 September 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Table of Contents  (Monday, 09/04/2023) Introduction to Alignment  Introduction to AI Alignment and Failure Cases  Discussion Questions   The Alignment Problem from a Deep Learning Perspective  Group of RL-based methods Group of LLM-based methods Group of Other ML methods     (Wednesday, 09/06/2023) Alignment Challenges and Solutions  Opening Discussion Introduction to Red-Teaming  In-class Activity (5 groups) How to use Red-Teaming?
<p class="text-right"><a href="/week2/">Read More…</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/2/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 1 of 2</span></li>      
      
    </ul>    
    All Posts by <a href="https://llmrisks.github.io/categories">Category</a> or <a href="https://llmrisks.github.io/tags">Tags</a>.

  </div>
</div>

</div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-12 medium-6">
      <a href="//llmrisks.github.io"><b>cs 6501: Risks (and Benefits) of Generative AI</b></a><br>
      Fall 2023<br>
      <a href="//www.cs.virginia.edu">University of Virginia</a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="https://llmrisks.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
  </hr>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
