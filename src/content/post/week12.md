+++
date = "20 Nov 2023"
draft = false
title = "Week 12: Regulating Dangerous Technologies"
slug = "week12"
+++

The slides are here: [Regulating Dangerous Technologies](https://www.dropbox.com/scl/fi/ycrjkoau5kclxq09ckvx4/regulation-post.pdf?rlkey=28sxdj7pf4pzlbjtavn59bufl&dl=0) (I've included some slides in the posted slides that I didn't present in class but you might find interesting, including some excerpts from a talk I gave in 2018 on [_Mutually Assured Destruction and the Impending AI Apocalypse_](https://speakerdeck.com/evansuva/mutually-assured-destruction-and-the-impending-ai-apocalypse).)

Since one of the groups made the analogy to tobacco products, I also will take the liberty of pointing to a talk I gave at Google making a similar analogy: [_The Dragon in the Room_](https://uvasrg.github.io/google-federated-privacy-2019-the-dragon-in-the-room/).

Stephanie made the point after class about how important individuals
making brave decisions is to things working out, in particular with
humanity (so far!) avoiding annihilating ourselves with nuclear
weapons. Stanislav Petrov may well have been the single person between
us and nuclear destruction in 1983, when he prevented an alert (which
he correctly determined was a false alarm) produced by the Soviet
detection system from going up the chain. Here's one (of many)
articles on this: [_'I Had A Funny Feeling in My
Gut'_](https://www.washingtonpost.com/wp-srv/inatl/longterm/coldwar/shatter021099b.htm),
Washington Post, 10 Feb 1999. There is still a lot of uncertainty and
skepticism if we should be fearing any kind of out-of-control AI risk,
but it is not so hard to imagine scenarios where our fate will
similarly come down to an individual's decision at a critical juncture.



